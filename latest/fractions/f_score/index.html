<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>F-Score · MLMetrics.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="MLMetrics.jl logo"/></a><h1>MLMetrics.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../gettingstarted/">Getting Started</a></li><li><a class="toctext" href="../../classification/">Classification Metrics</a><ul><li class="current"><a class="toctext" href>F-Score</a><ul class="internal"></ul></li></ul></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="../../classification/">Classification Metrics</a></li><li><a href>F-Score</a></li></ul><a class="edit-page" href="https://github.com/JuliaML/MLMetrics.jl/blob/master/docs/src/fractions/f_score.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>F-Score</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="F-Score-1" href="#F-Score-1">F-Score</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLMetrics.f_score" href="#MLMetrics.f_score"><code>MLMetrics.f_score</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">f_score(targets, outputs, [encoding], [avgmode], [beta = 1]) -&gt; Float64</code></pre><p>Compute the F-score for the <code>outputs</code> given the <code>targets</code>. The F-score is a measure for accessing the quality of binary predictor by considering both <em>recall</em> and the <em>precision</em>. Which value(s) denote &quot;positive&quot; or &quot;negative&quot; depends on the given (or inferred) <code>encoding</code>.</p><p>If <code>encoding</code> is omitted, the appropriate <code>MLLabelUtils.LabelEncoding</code> will be inferred from the types and/or values of <code>targets</code> and <code>outputs</code>. Note that omitting the <code>encoding</code> can cause performance penalties, which may include a lack of return-type inference.</p><p>The return value of the function depends on the number of labels in the given <code>encoding</code> and on the specified <code>avgmode</code>. In case an <code>avgmode</code> other than <code>:none</code> is specified, or the <code>encoding</code> is binary (i.e. it has exactly 2 labels), a single number is returned. Otherwise, the function will compute a separate result for each individual label, where that label is treated as &quot;positive&quot; and the other labels are treated as &quot;negative&quot;. These results are then returned as a single dictionary with an entry for each label.</p><p><strong>Arguments</strong></p><ul><li><p><code>targets::AbstractArray</code>: The array of ground truths <span>$\mathbf{y}$</span>.</p></li><li><p><code>outputs::AbstractArray</code>: The array of predicted outputs <span>$\mathbf{\hat{y}}$</span>.</p></li><li><p><code>encoding</code>: Optional. Specifies the possible values in <code>targets</code> and <code>outputs</code> and their interpretation (e.g. what constitutes as a positive or negative label, how many labels exist, etc). It can either be an object from the namespace <code>LabelEnc</code>, or a vector of labels.</p></li><li><p><code>avgmode</code>: Optional keyword argument. Specifies if and how class-specific results should be aggregated. This is mainly useful if there are more than two classes. Typical values are <code>:none</code> (default), <code>:micro</code> for micro averaging, or <code>:macro</code> for macro averaging. It is also possible to specify <code>avgmode</code> as a type-stable positional argument using an object from the <code>AvgMode</code> namespace.</p></li><li><p><code>beta::Number</code>: Optional keyword argument. Used to balance the importance of recall vs precision. The default <code>beta = 1</code> corresponds to the harmonic mean. A value of <code>beta &gt; 1</code> weighs recall higher than precision, while a value of <code>beta &lt; 1</code> weighs recall lower than precision.</p></li></ul><p><strong>See also</strong></p><p><a href="../accuracy/#MLMetrics.accuracy"><code>accuracy</code></a>, <a href="../positive_predictive_value/#MLMetrics.positive_predictive_value"><code>positive_predictive_value</code></a> (aka &quot;precision&quot;), <a href="../true_positive_rate/#MLMetrics.true_positive_rate"><code>true_positive_rate</code></a> (aka &quot;recall&quot; or &quot;sensitivity&quot;)</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; recall([1,0,0,1,1], [1,0,0,0,1])
0.6666666666666666

julia&gt; precision_score([1,0,0,1,1], [1,0,0,0,1])
1.0

julia&gt; f_score([1,0,0,1,1], [1,0,0,0,1])
0.8

julia&gt; f_score([1,0,0,1,1], [1,0,0,0,1], beta = 2)
0.7142857142857143

julia&gt; f_score([1,0,0,1,1], [1,0,0,0,1], beta = 0.5)
0.9090909090909091

julia&gt; f_score([1,0,0,1,1], [1,-1,-1,-1,1], LabelEnc.FuzzyBinary())
0.8

julia&gt; f_score([:a,:b,:a,:c,:c], [:a,:c,:b,:c,:c]) # avgmode=:none
Dict{Symbol,Float64} with 3 entries:
  :a =&gt; 0.666667
  :b =&gt; 0.0
  :c =&gt; 0.8

julia&gt; f_score([:a,:b,:a,:c,:c], [:a,:c,:b,:c,:c], avgmode=:micro)
0.6</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLMetrics.f1_score" href="#MLMetrics.f1_score"><code>MLMetrics.f1_score</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">f1_score(targets, outputs, [encoding], [avgmode])</code></pre><p>Same as <a href="#MLMetrics.f_score"><code>f_score</code></a>, but with <code>beta</code> fixed to 1.</p></div></div></section><footer><hr/><a class="previous" href="../diagnostic_odds_ratio/"><span class="direction">Previous</span><span class="title">Diagnostic Odds Ratio</span></a><a class="next" href="../false_discovery_rate/"><span class="direction">Next</span><span class="title">False Discovery Rate</span></a></footer></article></body></html>
