<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>F-Score · MLMetrics.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="MLMetrics.jl logo"/></a><h1>MLMetrics.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../gettingstarted/">Getting Started</a></li><li><a class="toctext" href="../../classification/">Classification Metrics</a><ul><li class="current"><a class="toctext" href>F-Score</a><ul class="internal"></ul></li></ul></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="../../classification/">Classification Metrics</a></li><li><a href>F-Score</a></li></ul><a class="edit-page" href="https://github.com/JuliaML/MLMetrics.jl/blob/master/docs/src/fractions/f_score.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>F-Score</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="F-Score-1" href="#F-Score-1">F-Score</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLMetrics.f_score" href="#MLMetrics.f_score"><code>MLMetrics.f_score</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">f_score(targets, outputs, [encoding], [avgmode], [β = 1]) -&gt; Float64</code></pre><p>Compute the F-score for the <code>outputs</code> given the <code>targets</code>. The F-score is a measure for accessing the quality of binary predictor by considering both <em>recall</em> and the <em>precision</em>.</p><pre><code class="language-julia-repl">julia&gt; recall([1,0,0,1,1], [1,0,0,0,1])
0.6666666666666666

julia&gt; precision_score([1,0,0,1,1], [1,0,0,0,1])
1.0

julia&gt; f_score([1,0,0,1,1], [1,0,0,0,1])
0.8</code></pre><p>The parameter <code>β</code> can be used to balance the importance of recall vs precision. The default <code>β = 1</code> corresponds to the harmonic mean. A value of <code>β &gt; 1</code> weighs recall higher than precision, while a value of <code>β &lt; 1</code> weighs recall lower than precision.</p><pre><code class="language-julia-repl">julia&gt; f_score([1,0,0,1,1], [1,0,0,0,1], 2)
0.7142857142857143

julia&gt; f_score([1,0,0,1,1], [1,0,0,0,1], 0.5)
0.9090909090909091</code></pre><p>The optional parameter <code>encoding</code> serves as specifcation of the existing labels and their interpretation (e.g. what constitutes as positive or negative, how many classes exist, etc). It can either be an object from the namespace <code>LabelEnc</code>, or a vector of labels. If omitted, the appropriate <code>encoding</code> will be inferred from the types and/or values of <code>targets</code> and <code>outputs</code>. Note that omitting the <code>encoding</code> can cause performance penalties, which may include a lack of type stability.</p><pre><code class="language-julia-repl">julia&gt; f_score([1,0,0,1,1], [1,-1,-1,-1,1], LabelEnc.FuzzyBinary())
0.8</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLMetrics.f1_score" href="#MLMetrics.f1_score"><code>MLMetrics.f1_score</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">f1_score(targets, outputs, [encoding], [avgmode])</code></pre><p>Same as <a href="#MLMetrics.f_score"><code>f_score</code></a>, but with <code>β</code> fixed to 1.</p></div></div></section><footer><hr/><a class="previous" href="../diagnostic_odds_ratio/"><span class="direction">Previous</span><span class="title">Diagnostic Odds Ratio</span></a><a class="next" href="../false_discovery_rate/"><span class="direction">Next</span><span class="title">False Discovery Rate</span></a></footer></article></body></html>
